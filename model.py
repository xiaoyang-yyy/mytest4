import pandas as pd
import numpy as np
import os
import sys
import joblib
import chardet  # ç”¨äºè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder  # æ›¿æ¢get_dummiesï¼Œè§£å†³ç¼–ç ä¹±ç å’Œç‰¹å¾åŒ¹é…é—®é¢˜
import warnings
warnings.filterwarnings('ignore')  # å±è”½æ— å…³è­¦å‘Š

# ===================== åŸºç¡€é…ç½®ä¸ç›®å½•æ£€æŸ¥ =====================
# å®šä¹‰è·¯å¾„ï¼ˆrå‰ç¼€é¿å…è½¬ä¹‰ï¼‰
file_path = r"D:\streamlit_env\student_data_adjusted_rounded.csv"
model_dir = r"D:\streamlit_env"

# æ£€æŸ¥å¹¶åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
if not os.path.exists(model_dir):
    os.makedirs(model_dir)
    print(f"ğŸ“ ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºï¼š{model_dir}")
else:
    print(f"ğŸ“ ç›®å½•å·²å­˜åœ¨ï¼š{model_dir}")

# ===================== å·¥å…·å‡½æ•°ï¼ˆè§£å†³ä¹±ç æ ¸å¿ƒï¼‰ =====================
def detect_file_encoding(file_path):
    """è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç ï¼Œé¿å…è¯»å–ä¹±ç """
    with open(file_path, 'rb') as f:
        raw_data = f.read(10240)  # è¯»å–å‰10KBæ£€æµ‹ç¼–ç 
    result = chardet.detect(raw_data)
    encoding = result['encoding']
    # å…¼å®¹å¸¸è§ç¼–ç åˆ«å
    if encoding == 'GB2312':
        encoding = 'GBK'
    elif encoding is None:
        encoding = 'utf-8-sig'
    return encoding

def safe_save_model(obj, path, compress_level=5):
    """å®‰å…¨ä¿å­˜æ¨¡å‹ï¼Œé¿å…å­—ç¬¦ç¼–ç ä¹±ç ï¼ŒåŒæ—¶æœ€å¤§åŒ–å‹ç¼©"""
    # ç¡®ä¿è·¯å¾„ä¸ºå­—ç¬¦ä¸²ä¸”ç¼–ç æ­£ç¡®
    if isinstance(path, str):
        path = path.encode('utf-8').decode('utf-8')
    # é«˜å‹ç¼©æ¯”ä¿å­˜ï¼ˆ1-9ï¼Œ5æ˜¯å¹³è¡¡å€¼ï¼Œ9å‹ç¼©æœ€å¤§ä½†ä¿å­˜ç¨æ…¢ï¼‰
    joblib.dump(obj, path, compress=compress_level)

# ===================== 1. åŠ è½½å¹¶æ ¡éªŒæ•°æ®ï¼ˆå½»åº•è§£å†³ä¹±ç ï¼‰ =====================
try:
    # è‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œå½»åº•è§£å†³CSVè¯»å–ä¹±ç 
    file_encoding = detect_file_encoding(file_path)
    df = pd.read_csv(file_path, encoding=file_encoding)
    print(f"âœ… è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç ï¼š{file_encoding}")
    
    # æ¸…ç†åˆ—åï¼ˆç»Ÿä¸€æ ¼å¼ï¼Œé¿å…ç¬¦å·/ç¼–ç é—®é¢˜ï¼‰
    df.columns = (
        df.columns
        .str.strip()
        .str.replace('ï¼ˆå°æ—¶ï¼‰', '', regex=False)
        .str.replace('ï¼ˆ', '(', regex=False)
        .str.replace('ï¼‰', ')', regex=False)
        .str.encode('utf-8').str.decode('utf-8')  # ç¡®ä¿åˆ—åç¼–ç æ­£ç¡®
    )
    
    # æ ¡éªŒå¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = ['å­¦å·', 'æ€§åˆ«', 'ä¸“ä¸š', 'æ¯å‘¨å­¦ä¹ æ—¶é•¿', 'ä¸Šè¯¾å‡ºå‹¤ç‡', 'æœŸä¸­è€ƒè¯•åˆ†æ•°', 'ä½œä¸šå®Œæˆç‡', 'æœŸæœ«è€ƒè¯•åˆ†æ•°']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"æ•°æ®æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—ï¼š{missing_cols}")
    
    # æ¸…æ´—æ•°æ®
    df = df[required_cols].dropna()
    numeric_cols = ['æ¯å‘¨å­¦ä¹ æ—¶é•¿', 'ä¸Šè¯¾å‡ºå‹¤ç‡', 'æœŸä¸­è€ƒè¯•åˆ†æ•°', 'ä½œä¸šå®Œæˆç‡', 'æœŸæœ«è€ƒè¯•åˆ†æ•°']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    df = df.dropna()
    
    # æ ¡éªŒæ•°æ®é‡ï¼ˆè‡³å°‘10æ¡æ‰èƒ½è®­ç»ƒï¼‰
    if len(df) < 10:
        raise ValueError(f"æœ‰æ•ˆæ•°æ®é‡è¿‡å°‘ï¼Œä»…{len(df)}æ¡ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
    
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œæœ‰æ•ˆæ•°æ®é‡ï¼š{len(df)}æ¡")
except Exception as e:
    print(f"âŒ æ•°æ®åŠ è½½/å¤„ç†å¤±è´¥ï¼š{str(e)}")
    sys.exit(1)  # æ›¿ä»£exit()ï¼Œæ›´å®‰å…¨çš„é€€å‡ºæ–¹å¼

# ===================== 2. è®­ç»ƒæ•°å€¼é¢„æµ‹æ¨¡å‹ï¼ˆè¿›ä¸€æ­¥è½»é‡åŒ–ï¼‰ =====================
try:
    X_reg = df[['æ¯å‘¨å­¦ä¹ æ—¶é•¿', 'ä¸Šè¯¾å‡ºå‹¤ç‡', 'æœŸä¸­è€ƒè¯•åˆ†æ•°', 'ä½œä¸šå®Œæˆç‡']]
    y_reg = df['æœŸæœ«è€ƒè¯•åˆ†æ•°']
    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
        X_reg, y_reg, test_size=0.2, random_state=42
    )
    # çº¿æ€§å›å½’æ¨¡å‹æœ¬èº«ä½“ç§¯æå°ï¼Œæ— éœ€é¢å¤–è½»é‡åŒ–
    reg_model = LinearRegression()
    reg_model.fit(X_train_reg, y_train_reg)
    # è¾“å‡ºæ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼ˆæ–°å¢ï¼‰
    reg_score = reg_model.score(X_test_reg, y_test_reg)
    print(f"âœ… æ•°å€¼é¢„æµ‹æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Œæµ‹è¯•é›†RÂ²å¾—åˆ†ï¼š{reg_score:.4f}")
except Exception as e:
    print(f"âŒ æ•°å€¼æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼š{str(e)}")
    sys.exit(1)

# ===================== 3. è®­ç»ƒåˆ†ç±»é¢„æµ‹æ¨¡å‹ï¼ˆæè‡´è½»é‡åŒ–+è§£å†³ç¼–ç ä¹±ç ï¼‰ =====================
try:
    # æ–°å¢æˆç»©ç­‰çº§æ ‡ç­¾ï¼ˆåŠæ ¼/ä¸åŠæ ¼ï¼‰
    df['æˆç»©ç­‰çº§'] = df['æœŸæœ«è€ƒè¯•åˆ†æ•°'].apply(lambda x: 1 if x >= 60 else 0)
    
    # æ ¡éªŒåˆ†ç±»æ ‡ç­¾æ˜¯å¦æœ‰ä¸¤ç§ï¼ˆé¿å…å…¨åŠæ ¼/å…¨ä¸åŠæ ¼ï¼‰
    if df['æˆç»©ç­‰çº§'].nunique() < 2:
        raise ValueError("æˆç»©ç­‰çº§ä»…æœ‰ä¸€ç§ï¼ˆå…¨åŠæ ¼/å…¨ä¸åŠæ ¼ï¼‰ï¼Œæ— æ³•è®­ç»ƒåˆ†ç±»æ¨¡å‹")
    
    # åˆ†ç¦»ç±»åˆ«ç‰¹å¾å’Œæ•°å€¼ç‰¹å¾ï¼ˆè§£å†³get_dummiesç¼–ç ä¹±ç é—®é¢˜ï¼‰
    cat_features = ['æ€§åˆ«', 'ä¸“ä¸š']
    num_features = ['æ¯å‘¨å­¦ä¹ æ—¶é•¿', 'ä¸Šè¯¾å‡ºå‹¤ç‡', 'æœŸä¸­è€ƒè¯•åˆ†æ•°', 'ä½œä¸šå®Œæˆç‡']
    
    # ä½¿ç”¨OneHotEncoderæ›¿ä»£get_dummiesï¼Œè§£å†³ç‰¹å¾ç¼–ç ä¹±ç å’ŒåŒ¹é…é—®é¢˜
    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    cat_encoded = encoder.fit_transform(df[cat_features])
    cat_feature_names = encoder.get_feature_names_out(cat_features)
    
    # æ‹¼æ¥ç‰¹å¾
    X_clf = np.hstack([df[num_features].values, cat_encoded])
    X_clf_df = pd.DataFrame(X_clf, columns=list(num_features) + list(cat_feature_names))
    y_clf = df['æˆç»©ç­‰çº§']
    
    X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
        X_clf, y_clf, test_size=0.2, random_state=42
    )
    
    # æè‡´è½»é‡åŒ–éšæœºæ£®æ—å‚æ•°ï¼ˆè¿›ä¸€æ­¥å‡å°ä½“ç§¯ï¼‰
    clf_model = RandomForestClassifier(
        n_estimators=30,        # æ ‘æ•°é‡ä»50å‡åˆ°30ï¼Œå¤§å¹…å‡å°ä½“ç§¯
        max_depth=8,            # æ ‘æ·±åº¦ä»10å‡åˆ°8
        min_samples_split=8,    # åˆ†è£‚æœ€å°æ ·æœ¬æ•°å¢åŠ 
        min_samples_leaf=3,     # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°å¢åŠ 
        max_features='sqrt',    # é™åˆ¶æ¯æ¬¡åˆ†è£‚ä½¿ç”¨çš„ç‰¹å¾æ•°
        n_jobs=1,               # å•çº¿ç¨‹è®­ç»ƒï¼Œå‡å°æ¨¡å‹ä½“ç§¯ï¼ˆå¤šçº¿ç¨‹ä¼šå¢åŠ åºåˆ—åŒ–ä½“ç§¯ï¼‰
        random_state=42,
        verbose=0
    )
    clf_model.fit(X_train_clf, y_train_clf)
    
    # è¾“å‡ºæ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼ˆæ–°å¢ï¼‰
    clf_score = clf_model.score(X_test_clf, y_test_clf)
    print(f"âœ… åˆ†ç±»é¢„æµ‹æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{clf_score:.4f}")
except Exception as e:
    print(f"âŒ åˆ†ç±»æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼š{str(e)}")
    sys.exit(1)

# ===================== 4. å‹ç¼©ä¿å­˜æ¨¡å‹ï¼ˆè§£å†³ä¹±ç +æœ€å°ä½“ç§¯ï¼‰ =====================
try:
    # ä¿å­˜æ•°å€¼é¢„æµ‹æ¨¡å‹ï¼ˆé«˜å‹ç¼©æ¯”ï¼‰
    reg_model_path = os.path.join(model_dir, "linear_regression_model.pkl")
    safe_save_model(reg_model, reg_model_path, compress_level=9)  # 9ä¸ºæœ€é«˜å‹ç¼©æ¯”
    print(f"âœ… æ•°å€¼æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{reg_model_path}")
    if os.path.exists(reg_model_path):
        reg_size = round(os.path.getsize(reg_model_path)/1024, 2)
        print(f"   âœ”ï¸ æ•°å€¼æ¨¡å‹å¤§å°ï¼š{reg_size} KB")
    
    # ä¿å­˜åˆ†ç±»é¢„æµ‹æ¨¡å‹ï¼ˆé«˜å‹ç¼©æ¯”ï¼‰
    clf_model_path = os.path.join(model_dir, "random_forest_clf.pkl")
    safe_save_model(clf_model, clf_model_path, compress_level=9)
    print(f"âœ… åˆ†ç±»æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{clf_model_path}")
    if os.path.exists(clf_model_path):
        clf_size = round(os.path.getsize(clf_model_path)/1024, 2)
        print(f"   âœ”ï¸ åˆ†ç±»æ¨¡å‹å¤§å°ï¼š{clf_size} KB")
    
    # ä¿å­˜ç¼–ç å™¨å’Œç‰¹å¾åˆ—åï¼ˆè§£å†³é¢„æµ‹æ—¶ç¼–ç ä¹±ç ï¼‰
    encoder_path = os.path.join(model_dir, "onehot_encoder.pkl")
    safe_save_model(encoder, encoder_path, compress_level=9)
    feature_cols_path = os.path.join(model_dir, "clf_feature_cols.pkl")
    safe_save_model(X_clf_df.columns, feature_cols_path, compress_level=9)
    print(f"âœ… ç¼–ç å™¨/ç‰¹å¾åˆ—åå·²ä¿å­˜ï¼Œè§£å†³é¢„æµ‹æ—¶ç¼–ç é—®é¢˜")

except Exception as e:
    print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥ï¼š{str(e)}")
    sys.exit(1)

# ===================== éªŒè¯æ–‡ä»¶ç”Ÿæˆ =====================
print("\nğŸ‰ æ‰€æœ‰æ¨¡å‹ä¿å­˜å®Œæˆï¼")
print("\nğŸ“‹ æ¨¡å‹ç›®å½•ä¸‹çš„.pklæ–‡ä»¶åˆ—è¡¨ï¼š")
total_size = 0
for file in os.listdir(model_dir):
    if file.endswith('.pkl'):
        file_path_full = os.path.join(model_dir, file)
        file_size = round(os.path.getsize(file_path_full)/1024, 2)
        total_size += file_size
        print(f"   - {file} | å¤§å°ï¼š{file_size} KB")
print(f"\nğŸ“Š æ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ€»å¤§å°ï¼š{total_size:.2f} KB")
